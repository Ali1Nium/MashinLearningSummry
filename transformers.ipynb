{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Fine-Tuning \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load pre-trained model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom layers on top\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)  # 10 classes\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=10, validation_data=(val_images, val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Feature Extraction\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load pre-trained model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "# Extract features\n",
    "features = model.predict(train_images)\n",
    "\n",
    "# Train a new classifier on extracted features\n",
    "clf = LogisticRegression()\n",
    "clf.fit(features, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Domain Adaptation \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load pre-trained model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)  # num_classes is the number of classes in target domain\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=10, validation_data=(val_images, val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Multi-Task Learning \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define inputs\n",
    "input_layer = Input(shape=(224, 224, 3))\n",
    "\n",
    "# Shared layers\n",
    "x = Flatten()(input_layer)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "\n",
    "# Task 1 output\n",
    "task1_output = Dense(num_classes_task1, activation='softmax', name='task1')(x)\n",
    "\n",
    "# Task 2 output\n",
    "task2_output = Dense(num_classes_task2, activation='softmax', name='task2')(x)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=input_layer, outputs=[task1_output, task2_output])\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss={'task1': 'categorical_crossentropy', 'task2': 'categorical_crossentropy'},\n",
    "              metrics={'task1': 'accuracy', 'task2': 'accuracy'})\n",
    "\n",
    "# Train\n",
    "model.fit(train_images, {'task1': train_labels_task1, 'task2': train_labels_task2}, epochs=10)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Knowledge Distillation \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50, MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Teacher model\n",
    "teacher_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = teacher_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "teacher_predictions = Dense(num_classes, activation='softmax')(x)\n",
    "teacher_model = Model(inputs=teacher_model.input, outputs=teacher_predictions)\n",
    "\n",
    "# Student model\n",
    "student_model = MobileNetV2(weights=None, include_top=False, input_shape=(224, 224, 3))\n",
    "x = student_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "student_predictions = Dense(num_classes, activation='softmax')(x)\n",
    "student_model = Model(inputs=student_model.input, outputs=student_predictions)\n",
    "\n",
    "# Compile student model\n",
    "student_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train student model\n",
    "student_model.fit(train_images, train_labels, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Meta-Learning\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchmeta.modules import MetaModule, MetaSequential\n",
    "from torchmeta.utils.gradient_based import get_inner_loop_optimizer\n",
    "\n",
    "class MetaLearner(MetaModule):\n",
    "    def __init__(self):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.model = MetaSequential(\n",
    "            torch.nn.Linear(10, 20),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(20, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "meta_learner = MetaLearner()\n",
    "optimizer = optim.Adam(meta_learner.parameters(), lr=0.001)\n",
    "inner_optimizer = get_inner_loop_optimizer(meta_learner.model, lr=0.01)\n",
    "\n",
    "# Training loop (pseudo-code)\n",
    "for epoch in range(num_epochs):\n",
    "    for task in tasks:\n",
    "        # Meta-training\n",
    "        meta_learner.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Task-specific training\n",
    "        for data in task:\n",
    "            # Inner-loop training\n",
    "            loss = meta_learner(data)\n",
    "            loss.backward()\n",
    "            inner_optimizer.step()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Self-Supervised Learning\n",
    " \n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Prepare data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "train_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Transfer Learning with GANs \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape\n",
    "\n",
    "# Define GAN model\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=100, activation='relu'))\n",
    "    model.add(Dense(784, activation='sigmoid'))\n",
    "    model.add(Reshape((28, 28, 1)))\n",
    "    return model\n",
    "\n",
    "# Load pre-trained GAN generator\n",
    "generator = build_generator()\n",
    "generator.load_weights('pretrained_gan_generator_weights.h5')\n",
    "\n",
    "# Generate images\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Few-Shot Learning \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrototypicalNetwork, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64)\n",
    "        )\n",
    "        self.classifier = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        prototypes = self.classifier(features)\n",
    "        return prototypes\n",
    "\n",
    "# Initialize and train\n",
    "model = PrototypicalNetwork()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (pseudo-code)\n",
    "for epoch in range(num_epochs):\n",
    "    for support_set, query_set in few_shot_tasks:\n",
    "        # Forward pass\n",
    "        prototypes = model(support_set)\n",
    "        # Compute loss and optimize\n",
    "        loss = compute_prototypical_loss(prototypes, query_set)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. Cross-Domain Transfer Learning\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load pre-trained model\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Replace the final layer\n",
    "num_features = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_features, num_classes)\n",
    "\n",
    "# Load data for new domain\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "dataset = CustomDataset(transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in dataloader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
