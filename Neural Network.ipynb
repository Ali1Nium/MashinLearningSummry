{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Working with Image Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://chatgpt.com/share/b1559c04-7e5b-44d6-acb0-89fe3f5510d8 \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the images to a [0, 1] range\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Reshape the data to add a channel dimension (needed for Conv2D)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "print(f'Predicted label for first test image: {np.argmax(predictions[0])}')\n",
    "\n",
    "model.save('mnist_cnn_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...................................................................................MODELS..........................................................................................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Feedforward Neural Network (FNN)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(output_dim, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Convolutional Neural Network (CNN) \n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recurrent Neural Network (RNN) \n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.SimpleRNN(128, input_shape=(timesteps, input_dim), activation='relu'),\n",
    "    layers.Dense(output_dim, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Long Short-Term Memory (LSTM)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.LSTM(128, input_shape=(timesteps, input_dim)),\n",
    "    layers.Dense(output_dim, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Gated Recurrent Unit (GRU) \n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.GRU(128, input_shape=(timesteps, input_dim)),\n",
    "    layers.Dense(output_dim, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Autoencoder \n",
    "input_img = layers.Input(shape=(input_dim,))\n",
    "encoded = layers.Dense(64, activation='relu')(input_img)\n",
    "decoded = layers.Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = models.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Variational Autoencoder (VAE) \n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "input_img = layers.Input(shape=(input_dim,))\n",
    "h = layers.Dense(64, activation='relu')(input_img)\n",
    "z_mean = layers.Dense(latent_dim)(h)\n",
    "z_log_var = layers.Dense(latent_dim)(h)\n",
    "\n",
    "z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "decoder_h = layers.Dense(64, activation='relu')\n",
    "decoder_mean = layers.Dense(input_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "vae = models.Model(input_img, x_decoded_mean)\n",
    "vae.compile(optimizer='adam', loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Generative Adversarial Network (GAN) \n",
    "# Generator\n",
    "generator = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_dim=latent_dim),\n",
    "    layers.Dense(784, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Discriminator\n",
    "discriminator = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Combined Model\n",
    "gan_input = layers.Input(shape=(latent_dim,))\n",
    "generated_image = generator(gan_input)\n",
    "gan_output = discriminator(generated_image)\n",
    "gan = models.Model(gan_input, gan_output)\n",
    "gan.compile(optimizer='adam', loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Transformer\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dense, Embedding, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=head_size, dropout=dropout)(inputs, inputs)\n",
    "    attn_output = LayerNormalization()(attn_output + inputs)\n",
    "    ffn_output = Dense(ff_dim, activation=\"relu\")(attn_output)\n",
    "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "    return LayerNormalization()(ffn_output + attn_output)\n",
    "\n",
    "inputs = Input(shape=(seq_length,))\n",
    "x = Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)\n",
    "x = transformer_encoder(x, head_size=embed_dim, num_heads=4, ff_dim=64, dropout=0.1)\n",
    "x = Dense(output_dim, activation=\"softmax\")(x)\n",
    "model = Model(inputs, x)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. Multilayer Perceptron (MLP) \n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(output_dim, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. Radial Basis Function Network (RBFN) \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RBFN(nn.Module):\n",
    "    def __init__(self, centers, out_dim):\n",
    "        super(RBFN, self).__init__()\n",
    "        self.centers = nn.Parameter(centers)\n",
    "        self.linear = nn.Linear(centers.size(0), out_dim, bias=True)\n",
    "    \n",
    "    def kernel_function(self, x, c):\n",
    "        return torch.exp(-torch.norm(x - c, dim=1)**2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        phi = torch.stack([self.kernel_function(x, c) for c in self.centers], dim=1)\n",
    "        return self.linear(phi)\n",
    "\n",
    "centers = torch.randn(10, input_dim)  # Example centers\n",
    "model = RBFN(centers, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. Self-Organizing Map (SOM) \n",
    "import numpy as np\n",
    "\n",
    "class SOM:\n",
    "    def __init__(self, m, n, dim, num_iterations=1000, learning_rate=0.5):\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.dim = dim\n",
    "        self.num_iterations = num_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.random.random((m, n, dim))\n",
    "    \n",
    "    def train(self, X):\n",
    "        for iteration in range(self.num_iterations):\n",
    "            for x in X:\n",
    "                bmu_idx = self.find_bmu(x)\n",
    "                self.update_weights(x, bmu_idx, iteration)\n",
    "    \n",
    "    def find_bmu(self, x):\n",
    "        diff = self.weights - x\n",
    "        dist = np.sum(diff**2, axis=-1)\n",
    "        return np.unravel_index(np.argmin(dist, axis=None), dist.shape)\n",
    "    \n",
    "    def update_weights(self, x, bmu_idx, iteration):\n",
    "        learning_rate = self.learning_rate * (1 - iteration / self.num_iterations)\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.n):\n",
    "                self.weights[i, j, :] += learning_rate * (x - self.weights[i, j, :])\n",
    "\n",
    "som = SOM(10, 10, input_dim)\n",
    "som.train(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13. Boltzmann Machine \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RBM(nn.Module):\n",
    "    def __init__(self, visible_dim, hidden_dim):\n",
    "        super(RBM, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(hidden_dim, visible_dim) * 0.1)\n",
    "        self.b = nn.Parameter(torch.zeros(visible_dim))\n",
    "        self.c = nn.Parameter(torch.zeros(hidden_dim))\n",
    "    \n",
    "    def forward(self, v):\n",
    "        h_prob = torch.sigmoid(F.linear(v, self.W, self.c))\n",
    "        h_sample = torch.bernoulli(h_prob)\n",
    "        v_prob = torch.sigmoid(F.linear(h_sample, self.W.t(), self.b))\n",
    "        return v_prob, h_sample\n",
    "\n",
    "rbm = RBM(visible_dim=input_dim, hidden_dim=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14. Deep Belief Network (DBN) \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DBN(nn.Module):\n",
    "    def __init__(self, visible_dim, hidden_dims):\n",
    "        super(DBN, self).__init__()\n",
    "        self.rbms = nn.ModuleList([RBM(visible_dim if i == 0 else hidden_dims[i-1], hidden_dim) for i, hidden_dim in enumerate(hidden_dims)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for rbm in self.rbms:\n",
    "            x, _ = rbm(x)\n",
    "        return x\n",
    "\n",
    "dbn = DBN(visible_dim=input_dim, hidden_dims=[128, 64])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15. Capsule Network (CapsNet) \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = tf.reduce_sum(tf.square(x), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + 1e-7)\n",
    "    return scale * x\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    def __init__(self, num_capsules, dim_capsules, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsules = num_capsules\n",
    "        self.dim_capsules = dim_capsules\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=[self.num_capsules, input_shape[-1], self.dim_capsules], initializer='glorot_uniform', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        u_hat = tf.einsum('...ji,kli->...kjl', inputs, self.W)\n",
    "        return squash(u_hat)\n",
    "\n",
    "inputs = layers.Input(shape=(input_dim,))\n",
    "x = layers.Dense(128)(inputs)\n",
    "capsule = CapsuleLayer(num_capsules=10, dim_capsules=16)(x)\n",
    "output = layers.Lambda(lambda x: tf.sqrt(tf.reduce_sum(tf.square(x), -1)))(capsule)\n",
    "model = models.Model(inputs, output)\n",
    "model.compile(optimizer='adam', loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16. Neural Turing Machine (NTM) \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NTM(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, memory_units, memory_vector_dim):\n",
    "        super(NTM, self).__init__()\n",
    "        self.controller = nn.LSTM(input_dim + memory_vector_dim, hidden_size, num_layers=1)\n",
    "        self.memory = torch.zeros(memory_units, memory_vector_dim)\n",
    "        self.read_head = torch.zeros(memory_vector_dim)\n",
    "        self.write_head = torch.zeros(memory_vector_dim)\n",
    "        self.fc = nn.Linear(hidden_size + memory_vector_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.cat((x, self.read_head), dim=-1)\n",
    "        out, _ = self.controller(x)\n",
    "        self.read_head = self.memory.mean(dim=0)  # Simplified memory read\n",
    "        output = self.fc(torch.cat((out, self.read_head), dim=-1))\n",
    "        return output\n",
    "\n",
    "ntm = NTM(input_dim=input_dim, output_dim=output_dim, memory_units=128, memory_vector_dim=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17. Attention Mechanism \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class Attention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "attention_layer = Attention(units=10)\n",
    "context_vector, attention_weights = attention_layer(query, values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#18. Spiking Neural Networks (SNN) \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SpikingNeuron:\n",
    "    def __init__(self, threshold=1.0, decay=0.9):\n",
    "        self.threshold = threshold\n",
    "        self.decay = decay\n",
    "        self.membrane_potential = 0.0\n",
    "    \n",
    "    def forward(self, input_signal):\n",
    "        self.membrane_potential = self.membrane_potential * self.decay + input_signal\n",
    "        if self.membrane_potential >= self.threshold:\n",
    "            self.membrane_potential = 0\n",
    "            return 1  # Spike\n",
    "        return 0  # No spike\n",
    "\n",
    "neuron = SpikingNeuron(threshold=1.0, decay=0.9)\n",
    "output_spike = neuron.forward(input_signal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#19. Graph Neural Networks (GNN)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = pyg_nn.GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = pyg_nn.GCNConv(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "model = GCN(input_dim=dataset.num_features, hidden_dim=16, output_dim=dataset.num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20. Neural Ordinary Differential Equations (Neural ODEs) \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, t, y):\n",
    "        return self.net(y)\n",
    "\n",
    "ode_func = ODEFunc()\n",
    "y0 = torch.tensor([[0.0, 1.0]])\n",
    "t = torch.linspace(0, 1, 100)\n",
    "solution = odeint(ode_func, y0, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21. Liquid State Machine (LSM)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LiquidStateMachine:\n",
    "    def __init__(self, num_neurons, connectivity=0.1):\n",
    "        self.num_neurons = num_neurons\n",
    "        self.connectivity = connectivity\n",
    "        self.weights = np.random.rand(num_neurons, num_neurons) * (np.random.rand(num_neurons, num_neurons) < connectivity)\n",
    "    \n",
    "    def forward(self, input_signal):\n",
    "        state = np.zeros(self.num_neurons)\n",
    "        for t in range(len(input_signal)):\n",
    "            state = np.dot(self.weights, state) + input_signal[t]\n",
    "            state = np.tanh(state)\n",
    "        return state\n",
    "\n",
    "lsm = LiquidStateMachine(num_neurons=100)\n",
    "output_state = lsm.forward(input_signal)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
